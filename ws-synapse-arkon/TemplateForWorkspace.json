{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "ws-synapse-arkon"
		},
		"ws-synapse-arkon-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ws-synapse-arkon-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:ws-synapse-arkon.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"ls_synapse_conection_properties_typeProperties_server": {
			"type": "string",
			"defaultValue": "ws-synapse-arkon.sql.azuresynapse.net"
		},
		"ls_synapse_conection_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "sql_relational_dwh"
		},
		"ws-synapse-arkon-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://blobstoragearkon.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/arkon_external_activity_to_dwh')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Raw Extraction and df exploration",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Copy into - bronce to SQL database",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "conector_to_sql_raw",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "arkonpool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null,
							"authentication": {
								"type": "MSI"
							}
						}
					},
					{
						"name": "Silver transform to Gold dwh",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Raw Extraction and df exploration",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "silver_layer_parquet",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "arkonpool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null,
							"authentication": {
								"type": "MSI"
							}
						}
					},
					{
						"name": "Copy into - bronce to SQL database",
						"type": "Script",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "ls_synapse_conection",
							"type": "LinkedServiceReference"
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": "SELECT TOP 100 * FROM dbo.sql_workers_raw;"
								}
							],
							"scriptBlockExecutionTimeout": "02:00:00"
						}
					},
					{
						"name": "Gold External Tables",
						"type": "Script",
						"dependsOn": [
							{
								"activity": "Gold data to dwh and sql",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "ls_synapse_conection",
							"type": "LinkedServiceReference"
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "NonQuery",
									"text": "IF EXISTS (\n    SELECT 1 FROM sys.external_tables WHERE name = 'workers_by_fullname'\n)\nBEGIN\n    DROP EXTERNAL TABLE workers_by_fullname;\nEND;\n\n-- Crear tabla externa workers_by_fullname\nCREATE EXTERNAL TABLE workers_by_fullname (\n    full_name VARCHAR(255),\n    company_name VARCHAR(255),\n    address VARCHAR(255),\n    city VARCHAR(100),\n    county VARCHAR(100),\n    state CHAR(2),\n    zip INT,\n    phone1 VARCHAR(20),\n    phone2 VARCHAR(20),\n    email VARCHAR(255),\n    web VARCHAR(255)\n)\nWITH (\n    LOCATION = 'silver_layer/workers_by_fullname.parquet',\n    DATA_SOURCE = blob_storage,\n    FILE_FORMAT = ParquetFormat\n);\n"
								}
							],
							"scriptBlockExecutionTimeout": "02:00:00"
						}
					},
					{
						"name": "Gold data to dwh and sql",
						"type": "Script",
						"dependsOn": [
							{
								"activity": "Silver transform to Gold dwh",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "ls_synapse_conection",
							"type": "LinkedServiceReference"
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": "SELECT TOP 100 * FROM dbo.sql_bigcities_dwh_gold;\n\nSELECT TOP 100 * FROM dbo.sql_byfullname_dwh_gold;\n\n"
								}
							],
							"scriptBlockExecutionTimeout": "02:00:00"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2025-07-12T03:58:36Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/conector_to_sql_raw')]",
				"[concat(variables('workspaceId'), '/bigDataPools/arkonpool')]",
				"[concat(variables('workspaceId'), '/notebooks/silver_layer_parquet')]",
				"[concat(variables('workspaceId'), '/linkedServices/ls_synapse_conection')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "ws-synapse-arkon-WorkspaceDefaultSqlServer",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/ws-synapse-arkon-WorkspaceDefaultSqlServer')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ls_synapse_conection')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"server": "[parameters('ls_synapse_conection_properties_typeProperties_server')]",
					"database": "[parameters('ls_synapse_conection_properties_typeProperties_database')]",
					"encrypt": "mandatory",
					"trustServerCertificate": false,
					"authenticationType": "SystemAssignedManagedIdentity"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws-synapse-arkon-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('ws-synapse-arkon-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws-synapse-arkon-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ws-synapse-arkon-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bigcities_sql_dwh_gold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP 100 * FROM dbo.sql_bigcities_dwh_gold\nGo",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sql_relational_dwh",
						"poolName": "sql_relational_dwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/byfullname_sql_dwh_gold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP 100 * FROM dbo.sql_byfullname_dwh_gold\nGo",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sql_relational_dwh",
						"poolName": "sql_relational_dwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/load_data_raw_csv')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP 100 * FROM dbo.sql_workers_raw\nGo\n\nSELECT COUNT(*) AS total_filas FROM dbo.sql_workers_raw;\nGO\n\nSELECT \n    SUM(CASE WHEN first_name IS NULL OR first_name = '' THEN 1 ELSE 0 END) AS nulos_nombre,\n    SUM(CASE WHEN email IS NULL OR email = '' THEN 1 ELSE 0 END) AS nulos_email,\n    SUM(CASE WHEN phone1 IS NULL OR phone1 = '' THEN 1 ELSE 0 END) AS nulos_telefono\nFROM dbo.sql_workers_raw;\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sql_relational_dwh",
						"poolName": "sql_relational_dwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/load_data_raw_parquet')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP 100 * FROM dbo.sql_workers_raw_parquet\nGo",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sql_relational_dwh",
						"poolName": "sql_relational_dwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/workers_fullname_dwh')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF EXISTS (\n    SELECT 1 FROM sys.external_tables WHERE name = 'workers_by_fullname'\n)\nBEGIN\n    DROP EXTERNAL TABLE workers_by_fullname;\nEND\nGO\n\n-- Ahora sí, crear la tabla\nCREATE EXTERNAL TABLE workers_by_fullname (\n    full_name VARCHAR(255),\n    company_name VARCHAR(255),\n    address VARCHAR(255),\n    city VARCHAR(100),\n    county VARCHAR(100),\n    state CHAR(2),\n    zip INT,\n    phone1 VARCHAR(20),\n    phone2 VARCHAR(20),\n    email VARCHAR(255),\n    web VARCHAR(255)\n)\nWITH (\n    LOCATION = 'silver_layer/workers_by_fullname.parquet',\n    DATA_SOURCE = blob_storage,\n    FILE_FORMAT = ParquetFormat\n);\nGO\n\nSELECT TOP 10 * FROM workers_by_fullname;\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sql_relational_dwh",
						"poolName": "sql_relational_dwh"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/conector_to_sql_raw')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Raw_scripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "arkonpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "a10a04e0-8b4f-4c13-9f40-131545f0a1b4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/242239df-ca9e-439a-b560-2b82454d87ff/resourceGroups/arkon-gr/providers/Microsoft.Synapse/workspaces/ws-synapse-arkon/bigDataPools/arkonpool",
						"name": "arkonpool",
						"type": "Spark",
						"endpoint": "https://ws-synapse-arkon.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/arkonpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from notebookutils import mssparkutils\n",
							"mssparkutils.credentials.help()\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from notebookutils import mssparkutils\n",
							"\n",
							"# Nombre del Azure Key Vault y del secreto\n",
							"akv_name = \"synapse-arkonkey\"\n",
							"secret_name = \"synapse-arkonpassdb\"\n",
							"\n",
							"# Obtener la contraseña desde Azure Key Vault\n",
							"try:\n",
							"    db_password = mssparkutils.credentials.getSecret(akv_name, secret_name)\n",
							"    print(\"✅ Contraseña obtenida desde Azure Key Vault\")\n",
							"except Exception as e:\n",
							"    raise RuntimeError(f\"❌ Error obteniendo el secreto '{secret_name}': {e}\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"jdbc_url = (\n",
							"    f\"jdbc:sqlserver://ws-synapse-arkon.sql.azuresynapse.net:1433;\"\n",
							"    f\"databaseName=sql_relational_dwh;\"\n",
							"    f\"user=franadmin01@ws-synapse-arkon;\"\n",
							"    f\"password={db_password};\"\n",
							"    f\"encrypt=true;\"\n",
							"    f\"trustServerCertificate=false;\"\n",
							"    f\"hostNameInCertificate=*.sql.azuresynapse.net;\"\n",
							"    f\"loginTimeout=30;\"\n",
							")\n",
							"\n",
							"print(\"🔗 Cadena de conexión lista:\")\n",
							"print(jdbc_url)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# Nombre de la tabla que quieres leer\n",
							"table_name = \"dbo.sql_workers_raw\"\n",
							"\n",
							"# Leer los datos desde el SQL Pool\n",
							"df = spark.read \\\n",
							"    .format(\"jdbc\") \\\n",
							"    .option(\"url\", jdbc_url) \\\n",
							"    .option(\"dbtable\", table_name) \\\n",
							"    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
							"    .load()\n",
							"\n",
							"# Mostrar resultados\n",
							"print(\"📊 Primeras filas:\")\n",
							"display(df.limit(20))\n",
							"\n",
							"total_rows = df.count()\n",
							"print(f\"\\n🔢 Total de filas: {total_rows}\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from pyspark.sql import DataFrame\n",
							"\n",
							"# Convertir a Parquet (en memoria)\n",
							"df_parquet = df  # El DataFrame ya está listo para usar como si fuera parquet\n",
							"\n",
							"# Opcional: Si quieres verificar el esquema o hacer operaciones encadenadas\n",
							"df_parquet.cache()  # Lo mantienes en memoria si planeas reusarlo"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"total_rows = df_parquet.count()\n",
							"print(f\"🔢 Total de filas: {total_rows}\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df_parquet.printSchema()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from pyspark.sql.functions import col\n",
							"\n",
							"# Contar valores nulos por columna\n",
							"null_counts = df_parquet.select(\n",
							"    [col(c).isNull().cast(\"int\").alias(c) for c in df_parquet.columns]\n",
							").first().asDict()\n",
							"\n",
							"print(\"❌ Valores nulos por columna:\")\n",
							"for col_name, count in null_counts.items():\n",
							"    print(f\"{col_name}: {count}\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/silver_layer_parquet')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Silver_scripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "arkonpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "9e81b19d-ee80-4f5d-bc2c-67a89b552a50"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/242239df-ca9e-439a-b560-2b82454d87ff/resourceGroups/arkon-gr/providers/Microsoft.Synapse/workspaces/ws-synapse-arkon/bigDataPools/arkonpool",
						"name": "arkonpool",
						"type": "Spark",
						"endpoint": "https://ws-synapse-arkon.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/arkonpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 15
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# EXTRACCIÓN E INGESTA\n",
							"# 1.- Transformación a parquet\n",
							"# 2.- Guardar en capa Silver"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# 1. Leer datos desde blob storage (ajusta la ruta según tu caso)\n",
							"input_path = \"abfss://bronce@blobstoragearkon.dfs.core.windows.net/Raw/datrip_raw.csv\"\n",
							"\n",
							"# Lee el archivo CSV\n",
							"df = spark.read.format(\"csv\") \\\n",
							"    .option(\"header\", \"true\") \\\n",
							"    .option(\"inferSchema\", \"true\") \\\n",
							"    .load(input_path)\n",
							"\n",
							"# 2. Mostrar solo las primeras 20 filas (versión visual)\n",
							"print(\"Mostrando las primeras 20 filas:\")\n",
							"display(df.limit(20))\n",
							"\n",
							"# 3. Guardar el DataFrame como Parquet\n",
							"output_path = \"abfss://silver@blobstoragearkon.dfs.core.windows.net/silver_layer/workers_cleaning.parquet\"\n",
							"\n",
							"df.write \\\n",
							"  .mode(\"overwrite\") \\\n",
							"  .parquet(output_path)\n",
							"\n",
							"print(f\"Archivo guardado exitosamente en: {output_path}\")"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# Leer el archivo Parquet recién guardado\n",
							"df_read_back = spark.read.parquet(output_path)\n",
							"\n",
							"# Mostrar los primeros registros\n",
							"print(\"Datos guardados:\")\n",
							"display(df_read_back.limit(5))"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# 1. Ruta del archivo original\n",
							"input_path = \"abfss://silver@blobstoragearkon.dfs.core.windows.net/silver_layer/workers_cleaning.parquet\"\n",
							"\n",
							"# 2. Leer el DataFrame desde Parquet\n",
							"df = spark.read.parquet(input_path)\n",
							"\n",
							"# 3. Crear full_name\n",
							"from pyspark.sql.functions import col, concat, lit\n",
							"\n",
							"df = df.withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
							"\n",
							"# 4. Reordenar las columnas para que 'full_name' esté primero\n",
							"columns = [\"full_name\"] + [col_name for col_name in df.columns if col_name not in [\"full_name\", \"first_name\", \"last_name\"]]\n",
							"\n",
							"df_reordered = df.select(columns)\n",
							"\n",
							"# 5. Eliminar first_name y last_name (ya no son necesarias)\n",
							"df_final = df_reordered.drop(\"first_name\", \"last_name\")\n",
							"\n",
							"# 6. Mostrar los primeros registros\n",
							"print(\"Datos con full_name al inicio:\")\n",
							"display(df_final.limit(5))\n",
							"\n",
							"# 7. Guardar como nuevo archivo Parquet\n",
							"output_path = \"abfss://silver@blobstoragearkon.dfs.core.windows.net/silver_layer/workers_by_fullname.parquet\"\n",
							"\n",
							"df_final.write \\\n",
							"    .mode(\"overwrite\") \\\n",
							"    .parquet(output_path)\n",
							"\n",
							"print(f\"\\nArchivo guardado exitosamente en: {output_path}\")"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Guardar en Gold"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Ruta en gold (directamente en la raíz)\n",
							"gold_output_path = \"abfss://gold@blobstoragearkon.dfs.core.windows.net/gold_layer/workers_by_fullname.parquet\"\n",
							"\n",
							"# Guardar en gold\n",
							"df_final.write \\\n",
							"    .mode(\"overwrite\") \\\n",
							"    .parquet(gold_output_path)\n",
							"\n",
							"print(f\"✅ Archivo guardado exitosamente en el contenedor gold: {gold_output_path}\")"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Partición (Opcional)"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# 1. Ruta del archivo original\n",
							"input_path = \"abfss://silver@blobstoragearkon.dfs.core.windows.net/silver_layer/workers_by_fullname.parquet\"\n",
							"\n",
							"# 2. Leer el DataFrame desde Parquet\n",
							"df = spark.read.parquet(input_path)\n",
							"\n",
							"# 3. Asegurarnos que las columnas city, county y state existen\n",
							"from pyspark.sql.functions import col\n",
							"\n",
							"df_clean = df.select(\n",
							"    col(\"full_name\"),\n",
							"    col(\"city\"),\n",
							"    col(\"county\"),\n",
							"    col(\"state\"),\n",
							"    col(\"company_name\"),\n",
							"    col(\"address\"),\n",
							"    col(\"zip\"),\n",
							"    col(\"phone1\"),\n",
							"    col(\"phone2\"),\n",
							"    col(\"email\"),\n",
							"    col(\"web\")\n",
							")\n",
							"\n",
							"# 4. Reordenamos para tener city, county, state al inicio\n",
							"df_reordered = df_clean.select(\"city\", \"county\", \"state\", *[col for col in df_clean.columns if col not in [\"city\", \"county\", \"state\"]])\n",
							"\n",
							"# 5. Definimos la ruta destino\n",
							"output_path = \"abfss://silver@blobstoragearkon.dfs.core.windows.net/silver_layer/by_cities/\"\n",
							"\n",
							"# 6. Escribimos particionando por city, county, state\n",
							"df_reordered.write \\\n",
							"    .partitionBy(\"city\", \"county\", \"state\") \\\n",
							"    .mode(\"overwrite\") \\\n",
							"    .parquet(output_path)\n",
							"\n",
							"print(f\"\\nDatos particionados y guardados exitosamente en: {output_path}\")"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Consulta de uno de los archivos particionados (Silver)\n",
							"Lo que se hizo fue separar la data por ciudad, condado y estado."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"city_to_read = \"Abilene\"\n",
							"path_to_read = f\"abfss://silver@blobstoragearkon.dfs.core.windows.net/silver_layer/by_cities/city={city_to_read}\"\n",
							"\n",
							"# Leer el parquet\n",
							"df_city = spark.read.parquet(path_to_read)\n",
							"\n",
							"# Recuperar la ciudad del path como columna\n",
							"from pyspark.sql.functions import lit\n",
							"\n",
							"df_city_with_name = df_city.withColumn(\"city\", lit(city_to_read))\n",
							"\n",
							"# Mostrar resultados\n",
							"display(df_city_with_name)"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Eliminación de errores de duplicado"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# 1. Ruta de entrada y salida\n",
							"input_path = \"abfss://silver@blobstoragearkon.dfs.core.windows.net/silver_layer/by_cities/\"\n",
							"output_path = \"abfss://silver@blobstoragearkon.dfs.core.windows.net/silver_layer/by_cities_cleaned/\"\n",
							"\n",
							"# 2. Leer todos los archivos parquet (incluyendo todas las particiones)\n",
							"df = spark.read.parquet(input_path)\n",
							"\n",
							"# 3. Eliminar filas duplicadas completamente iguales\n",
							"df_deduplicated = df.dropDuplicates()\n",
							"\n",
							"# 4. Mostrar algunos registros después de eliminar duplicados (opcional)\n",
							"print(\"Datos después de eliminar duplicados:\")\n",
							"display(df_deduplicated.limit(5))\n",
							"\n",
							"# 5. Guardar los datos limpios particionados por city, county y state\n",
							"df_deduplicated.write \\\n",
							"    .partitionBy(\"city\", \"county\", \"state\") \\\n",
							"    .mode(\"overwrite\") \\\n",
							"    .parquet(output_path)\n",
							"\n",
							"print(f\"\\nDatos limpios y guardados exitosamente en: {output_path}\")"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Hacer conteo de trabajadores de compañias por ciudad"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# 1. Ruta del directorio limpio\n",
							"cleaned_path = \"abfss://silver@blobstoragearkon.dfs.core.windows.net/silver_layer/by_cities_cleaned/\"\n",
							"\n",
							"# 2. Leer todos los archivos parquet\n",
							"df_cleaned = spark.read.parquet(cleaned_path)\n",
							"\n",
							"# 3. Agrupar por ciudad y contar registros\n",
							"from pyspark.sql.functions import count\n",
							"\n",
							"df_by_city = df_cleaned.groupBy(\"city\", \"county\", \"state\").agg(count(\"*\").alias(\"total_personas\"))\n",
							"\n",
							"# 4. Mostrar resultados\n",
							"print(\"\\nCantidad de personas por ciudad:\")\n",
							"display(df_by_city.orderBy(\"total_personas\", ascending=False))"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Dataframe de registros de New York"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# 1. Ruta del directorio limpio\n",
							"cleaned_path = \"abfss://silver@blobstoragearkon.dfs.core.windows.net/silver_layer/by_cities_cleaned/\"\n",
							"\n",
							"# 2. Leer todos los archivos parquet\n",
							"df_cleaned = spark.read.parquet(cleaned_path)\n",
							"\n",
							"# 3. Filtrar por ciudad = 'New York'\n",
							"df_ny = df_cleaned.filter(df_cleaned.city == \"New York\")\n",
							"\n",
							"# 4. Mostrar algunos registros\n",
							"print(\"\\nRegistros de New York:\")\n",
							"display(df_ny.limit(5))\n",
							"\n",
							"# 5. Contar total de registros de New York\n",
							"total_ny = df_ny.count()\n",
							"print(f\"\\nTotal de registros de New York: {total_ny}\")"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 1.- Separación de ciudades grandes.\n",
							"# 2.- Guardado en la capa Gold"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# 1. Ruta base de los datos limpios (silver)\n",
							"cleaned_path = \"abfss://silver@blobstoragearkon.dfs.core.windows.net/silver_layer/by_cities_cleaned/\"\n",
							"\n",
							"# 2. Leer todos los archivos Parquet\n",
							"df_cleaned = spark.read.parquet(cleaned_path)\n",
							"\n",
							"# 3. Definir las ciudades más grandes de EE.UU.\n",
							"big_cities = [\n",
							"    \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\",\n",
							"    \"Phoenix\", \"Philadelphia\", \"San Antonio\", \"San Diego\",\n",
							"    \"Dallas\", \"San Jose\"\n",
							"]\n",
							"\n",
							"# 4. Filtrar DataFrame para incluir solo esas ciudades\n",
							"df_big_cities = df_cleaned.filter(df_cleaned.city.isin(big_cities))\n",
							"\n",
							"# 5. Mostrar algunos registros para validar (opcional)\n",
							"print(\"\\nMostrando muestra de datos:\")\n",
							"display(df_big_cities.limit(10))\n",
							"\n",
							"# 6. Contar total de registros\n",
							"total_records = df_big_cities.count()\n",
							"print(f\"\\nTotal de registros de ciudades grandes: {total_records}\")\n",
							"\n",
							"# 7. Carpeta destino en gold (única carpeta para todas las ciudades)\n",
							"output_path = \"abfss://gold@blobstoragearkon.dfs.core.windows.net/gold_layer/all_big_cities.parquet\"\n",
							"\n",
							"# 8. Guardar como UN SOLO ARCHIVO PARQUET\n",
							"df_big_cities.coalesce(1).write \\\n",
							"    .mode(\"overwrite\") \\\n",
							"    .parquet(output_path)\n",
							"\n",
							"print(f\"\\nDatos de todas las ciudades grandes guardados en: {output_path}\")"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/arkonpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sql_relational_dwh')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westus2"
		}
	]
}